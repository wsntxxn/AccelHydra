defaults:
  - basic
  - data@batch: dynamic_batch_size
  - model: f5tts_base
  - loss@loss_fn: identity
  - _self_

n_mel_channels: 100

exp_name: f5tts_base_libritts
exp_dir: experiments/${exp_name}/
logging_file: ${exp_dir}/train.log

train_dataloader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: data_module.dataset.CustomDataset
    hf_data_raw_path: "./data/LibriTTS_100_360_500_char/raw.arrow"
    duration_path: "./data/LibriTTS_100_360_500_char/duration.json"
    min_duration: 0.3
    max_duration: 30.0
    target_sample_rate: 24000
    n_mel_channels: ${n_mel_channels}
    hop_length: 256
    win_length: 1024
    n_fft: 1024
    mel_spec_type: ${vocoder}
  batch_size: ${batch.batch_size}
  batch_sampler: ${batch.batch_sampler}
  num_workers: 12
  collate_fn:
    _target_: data_module.collate_function.F5Collate

val_dataloader: Null

warmup_params:
  warmup_steps: 20000
  warmup_epochs: Null
  epoch_length: ${epoch_length}

gradient_accumulation_steps: 1

optimizer:
  _target_: torch.optim.AdamW
  lr: !!float 7.5e-5
  weight_decay: 0.01

lr_scheduler:
  _target_: "transformers.get_scheduler"
  name: "linear"

epochs: 600
epoch_length: 2000

trainer:
  _target_: trainer.f5tts_trainer.F5TTSTrainer
  project_dir: ${exp_dir}
  logging_file: ${logging_file}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  max_grad_norm: 1.0
  epochs: ${epochs}
  epoch_length: ${epoch_length}
  save_last_k: 2
  permanent_save_every_n_steps: 100000
  early_stop: Null
  logging_config:
    _target_: accel_hydra.trainer.LoggingConfig
    report_to: wandb
    project: f5tts_base_libritts
    # project: runs
    save_dir: ${exp_dir}
    name: ${exp_name}
    resume_id: Null
    workspace: wsntxxn
  metric_monitor:
    _target_: accel_hydra.trainer.MetricMonitor
    metric_name: loss
    mode: min
  even_batches: ${batch.even_batches}